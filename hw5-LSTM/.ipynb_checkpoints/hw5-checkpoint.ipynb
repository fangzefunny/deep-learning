{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM gradient, Here you will derive the backprob updates for the univariate version of the LSTM, i.e., all input, hidden state, and output are univaraiate variables. For reference, below is a LSTM unit at time t and the computations it performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three types of binary gates are created\n",
    "\n",
    "* Forget gate: $f_t = \\sigma( W^{hf}h_{t-1} +W^{xf}X_t + W^f_0)$\n",
    "* Memory gate: $i_t = \\sigma( W^{hi}h_{t-1} + W^{xi}X_t + W^i_0)$\n",
    "* Output gate: $o_t = \\sigma( W^{ho}h_{t-1} + W^{xo}X_t + W^o_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information generation via gating\n",
    "\n",
    "* Intermediate memory content generation: $\\tilde{C}_t = \\text{tanh}( W^{hc}h_{t-1} + W^{xc}X_t + W^c_0)$\n",
    "* Current memory content generation via gating: $C_t = f_t \\otimes C_{t-1} \\oplus i_t \\otimes \\tilde{C}_t$\n",
    "* Current state generation via gating: $h_t = o_t \\otimes \\text{tanh}(C_t)$\n",
    "* Ouput Generation: $y_t = \\sigma( W^y h_t + W^y_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the gradient for the output $y$ at time t to be $\\bigtriangledown y_t$\n",
    "\n",
    "All these variables are scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Drive the gradient for the three gates\n",
    "\n",
    "To obtain the gradient of the gates, we should first know the gradient of $\\bigtriangledown h_t$, $\\bigtriangledown C_t$.\n",
    "Because the diagram showed there are next step, so we can assume that $t < T$, where $T$ is the last step of time sequence. \n",
    "\n",
    "Let's first consider the dimension. Since the output layer is a sigmoid function, $y_t$ should be a scalar. $h_t$ is also a scalar, and $w^y$ and $w^y_0$ should also be scalar. \n",
    "\n",
    "##### 1.1 Compute the gradient of $h_t$\n",
    "\n",
    "Because this equation: $y_t = \\sigma( z^y_t)$ and $z^y_t = W^y h_t + W^y_0$\n",
    "\n",
    "The gradient is: \n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown h_t =& \\frac{ \\partial y_t} {\\partial h_t} \\bigtriangledown y_t + \\frac{ \\partial f_{t+1}} {\\partial h_t} \\bigtriangledown f_{t+1} +\\\\& \\frac{ \\partial i_{t+1}} {\\partial h_t} \\bigtriangledown i_{t+1} + \\frac{ \\partial o_{t+1}} {\\partial h_t} \\bigtriangledown o_{t+1} + \\frac{ \\partial \\tilde{C}_{t+1}} {\\partial h_t} \\bigtriangledown \\tilde{C}_{t+1} \\\\ =& w^y y_t( 1 - y_t)  \\bigtriangledown y_t + \\frac{ \\partial f_{t+1}} {\\partial h_t} \\bigtriangledown f_{t+1} + \\\\&\\frac{ \\partial i_{t+1}} {\\partial h_t} \\bigtriangledown i_{t+1} + \\frac{ \\partial o_{t+1}} {\\partial h_t} \\bigtriangledown o_{t+1} + \\frac{ \\partial \\tilde{C}_{t+1}} {\\partial h_t} \\bigtriangledown \\tilde{C}_{t+1}\n",
    "\\end{align}\n",
    "\n",
    "However, it seems like we do not need to consider the next level.\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown h_t =& \\frac{ \\partial y_t} {\\partial h_t} \\bigtriangledown y_t  \\\\ =& w^y y_t( 1 - y_t)  \\bigtriangledown y_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Compute the gradient of $\\tilde{C}_t$\n",
    "\n",
    "Because this equation: $C_t = o_t \\otimes \\text{tanh}(C_t)$.\n",
    "\n",
    "In this univariate condition, we can rewrite this equation as: $h_t = o_t \\times \\text{tanh}(C_t)$.\n",
    "\n",
    "To help us obtain the gradient we add an auxilliary variable $z^c_t = \\frac {h_t} {o_t} = \\text{tanh}(C_t)$\n",
    "\n",
    "The gradient is: \n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown C_t =& \\frac{ \\partial h_t} {\\partial C_t} \\bigtriangledown h_t +\\frac{ \\partial C_{t+1}} {\\partial C_{t}} \\bigtriangledown C_{t+1}  \\\\=& \\frac{ \\partial z^c_t} {\\partial C_t} \\frac{ \\partial h_t} {\\partial z^c_t}  \\bigtriangledown h_t  + f_t  \\bigtriangledown C_{t+1} \\\\ =& o_t ( 1 - (\\text{tanh}(C_t))^2)  \\bigtriangledown h_t + f_t  \\bigtriangledown C_{t+1}\n",
    "\\end{align}\n",
    "\n",
    "If we do not need to consider the next level.\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown h_t =& \\frac{ \\partial h_t} {\\partial C_t} \\bigtriangledown h_t  \\\\ =& o_t ( 1 - (\\text{tanh}(C_t))^2)  \\bigtriangledown h_t \\\\=& o_t ( 1 - (\\text{tanh}(C_t))^2)  w^y y_t( 1 - y_t)  \\bigtriangledown y_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Compute the gradient of the gates\n",
    "\n",
    "Since $h_t = o_t \\otimes \\text{tanh}(C_t) \\Rightarrow h_t = o_t \\times \\text{tanh}(C_t)$:\n",
    "\n",
    "The $\\bigtriangledown o_t = \\frac {\\partial h_t} {\\partial o_t} \\bigtriangledown h_t = \\text{tanh}(C_t) \\bigtriangledown h_t = \\text{tanh}(C_t)w^y y_t( 1 - y_t)  \\bigtriangledown y_t$\n",
    "\n",
    "Since $C_t = f_t \\otimes C_{t-1} \\oplus i_t \\otimes \\tilde{C}_t \\Rightarrow f_t \\times C_{t-1} + i_t \\times \\tilde{C}_t$:\n",
    "\n",
    "The $\\bigtriangledown f_t = \\frac {\\partial c_t} {\\partial f_t} \\bigtriangledown c_t = C_{t-1} \\bigtriangledown C_t = C_{t-1}o_t ( 1 - (\\text{tanh}(C_t))^2)  w^y y_t( 1 - y_t)  \\bigtriangledown y_t$\n",
    "\n",
    "The $\\bigtriangledown i_t = \\frac {\\partial c_t} {\\partial i_t} \\bigtriangledown c_t = \\tilde{C}_t \\bigtriangledown C_t = \\tilde{C}_t o_t ( 1 - (\\text{tanh}(C_t))^2)  w^y y_t( 1 - y_t)  \\bigtriangledown y_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Derive the gradient for the weight $W^{hi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the memory gate, we use: $i_t = \\sigma( w_t^{hi}h_{t-1} + w_t^{xi}X_t + w^i_{t,0})$\n",
    "\n",
    "We can add an auxillary variable $z^i_t = w_t^{hi}h_{t-1} + w_t^{xi}X_t + w^i_{t,0}$\n",
    "\n",
    "So the equation becomes: $i_t = \\sigma(z^i_t)$\n",
    "\n",
    "The gradient becomes: \n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown w_t^{hi} =& \\frac{ \\partial i_t} {\\partial w^{hi}} \\bigtriangledown i_t \\\\=& \\frac{ \\partial z^i_t} {\\partial C_t} \\frac{ \\partial i_t} {\\partial z^i_t}  \\bigtriangledown i_t \\\\ =& h_{t-1} i_t(1-i_t)  \\bigtriangledown i_t \\\\=& h_{t-1} i_t(1-i_t) \\tilde{C}_t o_t ( 1 - (\\text{tanh}(C_t))^2)  w^y y_t( 1 - y_t)  \\bigtriangledown y_t\n",
    "\\end{align}\n",
    "\n",
    "Hence we need to sum over time, to caculate $w^{hi}$\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown w_t^{h} = \\sum_{i=1}^T h_{t-1} i_t(1-i_t) \\tilde{C}_t o_t ( 1 - (\\text{tanh}(C_t))^2)  w^y y_t( 1 - y_t)  \\bigtriangledown y_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Based on your answers above, explain why the gradient don't explode if the value of the forget gates are very close to 1. and the values of the memory gates are very close to  0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the value of the forget gates are close to 1 and memory gates are close to 0, whici means $f_t \\rightarrow 1, i_t \\rightarrow 0$. Then $c_t \\approx c_{t+1}$, the memory cell will always be close to the initial value, which should be a small number, due to we usually initiate the network with a small value. All the $h_t$ should also be a small value. Because $h_{t} = o_{t} \\otimes \\text{tanh}(C_{t})$, $o_t \\in [0,1]$ as the output of sigmoid, $\\tanh(C_t) \\in [-1,1]$ as the output of tanh function. $\\tilde{C}_t = \\text{tanh}( W^{hc}h_{t-1} + W^{xc}X_t + W^c_0)$ shouldn't be large if we carefully preprocessed the input. Based on all these, if we look at the $\\bigtriangledown w_t^h$ equation, $h_{t-1}$ is small, $i \\rightarrow 0$, $\\tilde{C}_t$ is small, $o_t \\in [0,1]$, $1-(\\text{tanh}(C_t))^2) \\in [0,1]$ $y_t(1-y_t) \\in [0, 1/4]$, so the gradient will not explode. \n",
    "\n",
    "The same conclusion should be able to be applied to other gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
