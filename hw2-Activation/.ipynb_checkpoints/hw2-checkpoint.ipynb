{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In the class, we derived the closed‐form solution for solving the parameters of the linear regression with one output. In this problem, you will apply the same technique to derive the equations for learning the parameters of linear regression with two outputs $y \\in R^2$ and $y=(y_1,y_2)^{\\top}$ jointly. Given training data $D=\\{x[m],y[m]\\}, m=1,2,..,M$, derive the equations to learn the regression parameter matrix $W=[W_1, W_2]$, where $W_1=[w_1, w_{1,0}]^{\\top}$ and $W_2=[w_2, w_{2,0}]^{\\top}$ by minimizing the mean squared errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume each sample $x[m]$ has N features： \n",
    "\n",
    "so $x[m] \\in R^{N \\times 1}$, $W \\in R^{(N+1) \\times 2}$, $y \\in R^{2 \\times 1}$\n",
    "\n",
    "The regression is: \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}[m] &= W^{\\top} \\left[ \\begin{matrix} x[m]\\\\ 1 \\end{matrix} \\right] \\\\&= W^{\\top}X[m]\n",
    "\\end{align}\n",
    "\n",
    "The loss function is:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(D;W) &= \\frac{1} {2} \\sum_{i=1}^M (y[m] - W^{\\top}X[m])^{\\top}(y[m] - W^{\\top}X[m]) \\\\&= \\frac{1} {2}\\sum_{i=1}^M (y[m])^{\\top}y[m] - (W^{\\top}X[m])^{\\top}y[m] - (y[m])^{\\top}W^{\\top}X[m] +  (W^{\\top}X[m]))^{\\top} W^{\\top}X[m] \\\\&= \\frac{1} {2}\\sum_{i=1}^M (y[m])^{\\top}y[m] - 2(W^{\\top}X[m])^{\\top}y[m] +  (W^{\\top}X[m]))^{\\top} W^{\\top}X[m] \\\\ &= \\frac{1} {2}\\sum_{i=1}^M (y[m])^{\\top}y[m] - 2X[m]^{\\top}Wy[m] +  (W^{\\top}X[m]))^{\\top} W^{\\top}X[m]\n",
    "\\end{align}\n",
    "\n",
    "To minimize the loss function, we want the gradient $\\bigtriangledown_{W}\\mathcal{L}(D;W) = 0$ \n",
    "\n",
    "The close-form solution is:\n",
    "\\begin{align}\n",
    "\\bigtriangledown_{W}\\mathcal{L}(D;W) = &\\sum_{i=1}^M - \\frac {\\partial 2(W^{\\top}X[m])^{\\top}y[m]}{\\partial W} +  \\frac {\\partial (W^{\\top}X[m]))^{\\top} W^{\\top}X[m])} {\\partial W} \\\\ = &\\sum_{i=1}^M - 2X[m]y^{\\top} +  2X[m]X[m]^{\\top} W \\\\ \\Rightarrow &\\sum_{i=1}^M 2X[m]y^{\\top} = \\sum_{i=1}^M 2X[m]X[m]^{\\top} W \\\\ \\Rightarrow &W = \\sum_{i=1}^M (X[m]X[m]^{\\top})^{-1}X[m]y^{\\top}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For binary classification with logistic regression classification, where $y\\in\\{+1,-1\\}$, we introduced the sigmoid function, where $\\sigma()$ is the sigmoid function. For binary classification with $y\\in\\{1,0\\}$ and $x\\in R^N$, we may use a new activation function such that  $p(y|x) = \\phi(y*(w^{\\top}x+w_0))$, where $phi()$ is defined as:\n",
    "$$\\phi( y( w^{\\top}x + w_0)) = \\frac {e^{y(w^{\\top}x +w_0)}} {1 + e^{(w^{\\top}x +w_0)}}$$\n",
    "### Given trainig data $D = \\{x[m],y[m]\\}, m=1,2,...,M$, derive the gradient descent solution to parameters $\\Theta$, where $\\Theta = \\left[ \\begin{matrix} w \\\\w_0 \\end{matrix} \\right]$ using the negative conditional log likelihood (cross-entropy) as the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote $\\Theta = \\left[ \\begin{matrix} w \\\\ w_0 \\end{matrix} \\right]$, $X = \\left[ \\begin{matrix} x \\\\ 1 \\end{matrix} \\right]$, so $w^{\\top}x + w_0 = X^{\\top}\\Theta$\n",
    "\n",
    "and denote scalar $z = X^{\\top}\\Theta$\n",
    "\n",
    "Then the activation function is $p(y|x): p = \\phi(yz) = \\frac {e^{yz}} {1+e^z}$\n",
    "\n",
    "The loss function $\\mathcal{L}(D,\\Theta) = - \\log p(y|x) = -\\log \\phi(yX^{\\top}\\Theta) = -\\log \\frac {e^{yX^{\\top}\\Theta}} {1+e^{X^{\\top}\\Theta}}$\n",
    "\n",
    "The gradient: \n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown_{\\Theta} \\mathcal{L}(D,\\Theta)= &\\frac {\\partial \\mathcal{L}} {\\partial p} \\frac {\\partial p} {\\partial z} \\frac {\\partial z}{\\partial \\Theta} \\\\= & - \\frac {1} {p} \\left( \\frac{\\partial e^{yz}}{\\partial z}\\frac{\\partial z}{\\partial \\Theta}(1+e^z)^{-1} + e^{yz}\\frac{\\partial (1+e^{z})^{-1}}{\\partial z}\\frac{\\partial z}{\\partial \\Theta}\\right)  \\\\= & - \\frac {1} {p} \\left( ye^{yz}\\frac{\\partial z}{\\partial \\Theta}(1+e^z)^{-1} + e^{yz}( - (1+e^{z})^{-2} e^{z})\\frac{\\partial z}{\\partial \\Theta}\\right) \\\\= & - \\frac {1} {e^{yz}(1+e^z)^{-1}} \\left( ye^{yz}(1+e^z)^{-1}X - e^{yz} (1+e^{z})^{-2}e^{z}X\\right) \\\\= & - \\frac {1} {e^{yz}(1+e^z)^{-1}}  e^{yz}(1+e^z)^{-1}\\left( y -  (1+e^{z})^{-1}e^{z}\\right) X\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
