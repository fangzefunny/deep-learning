{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input X is a 7 x 7 image. The convolution layer C is produced by convolving X with a 3x3 filter $W^x$ with stride 1, plus bias matrix $W^x_0$. The activation layer A is produced by applying the ReLU activation function to C. A max pooling of 3x3 with a stride 2 is then applied to C to produce the pooling layer P. A fully connected vector layer $\\vec{P}$ is then produced by concatenating rows of P. The output layer consists of one node y. It is produced by $y = \\sigma( (W^0)^{\\top}\\vec{P} + W^o_0))$, where $W^o$ and $W^o_0$ are output layer weight matrix and bias vector repectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Determine the dimension for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From input image to convolution layer**\n",
    "\n",
    "The input of the NN $X$ is a $7 \\times 7 $ matrix. The filter for the convolution has dimension $3 \\times 3$, and the stride of the covolution is 1. So we have the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "C(r,c) =& X * W^x_0(r,c) + w^x_0 \\\\=& \\sum_{i=1}^K \\sum_{j=1}^K X( (r+i) \\times s-1, (c+j)) \\times s-1) W( i,j) + W_0  \\\\ =& \\sum_{i=1}^3 \\sum_{j=1}^3 X( r+i-1,c+j-1)W(i, j) + W_0\n",
    "\\end{align}\n",
    "\n",
    "Because we are not instructed to add zero-padding to the image, The image should shrink to image with width of the image as $\\frac {N-K} {s} +1 = \\frac {7-3} {1} +1= 5$. So the convolution image should be a $5 \\times 5$ image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From convolution layer to activation layer**\n",
    "\n",
    "Since the activation function takes in a scalar and output a scalar, the size of the activation image $A$ should have the same size with the convolution image. So $A^{ N^a_r \\times N^a_c}$ is a $5 \\times 5$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From activation layer to pooling layer**\n",
    "\n",
    "In the pooling layer, we shrink a $d \\times d$ part of image to 1 pxiels by averaging the pxiel values in this $d \\times d$ part.\n",
    "\n",
    "\\begin{align}\n",
    "P(r,c) =& \\max_{1\\leq i \\leq d \\\\ 1\\leq j \\leq d} A( (r-1)\\times s +i, (c-1) \\times s +j ) \\\\=& \\max_{1\\leq i \\leq 3 \\\\ 1\\leq j \\leq 3} A( (r-1)\\times 2 +i, (c-1) \\times 2 +j ) \n",
    "\\end{align}\n",
    "\n",
    "The heigth and width of the pooling image is shrinked to $\\frac {N^a_r - d} {s} +1 = \\frac {5 - 3}  {2} +1 = 2$. So the pooling image is a $2 \\times 2$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From pooling layer to fully connected layer**\n",
    "\n",
    "In this layer, we sketch the image into a vector, so the dimsion of $\\vec{P}$ is $4 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From fully conncected layer to output layer**\n",
    "\n",
    " Since this is a binary classification problem, using sigmoid function as the activation function. The output layer is a scalar which tells the probability of outputing 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform forward propagation layer by layer to compute the values for each layer, the estimated output value $\\hat{y}$, and the gradient of the output $\\bigtriangledown \\hat{y}$, given y=1, using negative log conditional likelihood loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C image is:\n",
    "\n",
    "$$\\left[ \\begin{matrix} 0 & 0.8 & 0.7 & 0.6 & 0 \\\\0 & 0.8 & 0.7 & 0.6 & 0 \\\\0 & 0.8 & 0.7 & 0.6 & 0 \\\\0 & 0.8 & 0.7 & 0.6 & 0 \\\\0 & 0.8 & 0.7 & 0.6 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A image is: still this matrix, because all the value is larger than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pooling layer P is:\n",
    "\n",
    "$$\\left[ \\begin{matrix} 0.8 & 0.7 \\\\ 0.8  & 0.7 \\end{matrix} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fulling conncected layer stretchs the pooling layer $P$ into a vector $\\vec{P}$:\n",
    "\n",
    "$$\\vec{P} = [ 0.8,  0.7, 0.8, 0.7 ]^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer:\n",
    "    \n",
    "$$z = (W^o)^{\\top} \\vec{P} + W^o_0 = 0.75 - 0.2 = 0.55$$\n",
    "$$\\hat{y} = \\frac {\\exp(z)} {\\exp(z) +1 } = .634$$\n",
    "\n",
    "And $\\hat{y}$ is the probability of getting 1 as the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function:\n",
    "\n",
    "$$-\\log p(y|x) = - y \\log \\hat{y} = .456$$\n",
    "    \n",
    "So the gradient of the output $\\bigtriangledown \\hat{y} = -\\frac {y} {\\hat{y}} =- \\frac {1} {.634} = -1.58$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Given graident of $\\hat{y}$, perform back-propagation to obtain the weight matrix gradient $\\bigtriangledown W^o$ and bias vector gradient $\\bigtriangledown W^o_0$ for the output layer and the weight matrix gradient $\\bigtriangledown W^x$ and bias matrix gradient $\\bigtriangledown W^x_0$ respectively for the convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From output layer to fully connected layer**\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown W^o =& \\frac { \\partial z} {\\partial w^o} \\frac{\\partial \\hat{y}} {\\partial z} \\bigtriangledown \\hat{y} \\\\ =& \\vec{P} \\hat{y}( 1 - \\hat{y} ) -\\frac{y}{\\hat{y}} \\\\=& \\left[ \\begin{matrix} .8 & .7 & .8 &.7 \\end{matrix} \\right]^{\\top} * .634( 1- .634) * ( - 1.58) \\\\=& \\left[ \\begin{matrix} -.293 & -.257 & -.293 & -.257 \\end{matrix} \\right]^{\\top}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown W_0^o =& \\frac { \\partial z} {\\partial w_0^o} \\frac{\\partial \\hat{y}} {\\partial z} \\bigtriangledown \\hat{y} \\\\ =& 1 * \\hat{y}( 1 - \\hat{y} ) -\\frac{y}{\\hat{y}} \\\\=&  .634( 1- .634) * ( - 1.58) \\\\=& -.367\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From fully connected layer to max pooling layer**\n",
    "\n",
    "$$\\bigtriangledown_P = \\left[ \\begin{matrix} -.293 & -.257 \\\\ -.293 & -.257 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From max pooling to activation**\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown A[ (r-1) \\times s +i] [ (c-1)\\times s + j ] = \\left\\{  \\bigtriangledown P[r][c] \\text{ if } i =i*[r] \\text{ and } j = j*[c] \\atop 0 \\text{                otherwise } \\right. \n",
    "\\end{align}\n",
    "\n",
    "So the $\\bigtriangledown A$ is:\n",
    "\n",
    "$$\\bigtriangledown C =  \\left[ \\begin{matrix} 0 & -.293 & -.257 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & -.293 & -.257 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\\\0 &0 & 0 & 0 & 0\\end{matrix} \\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From activation to convolution layer**\n",
    "\n",
    "$$\\bigtriangledown C =  \\left[ \\begin{matrix} 0 & -.293 & -.257 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & -.293 & -.257 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\\\0 &0 & 0 & 0 & 0\\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the convolution layer to input layer**\n",
    "\n",
    "\\begin{align}\n",
    "\\bigtriangledown W^x =& \\frac {\\partial C} {\\partial W^x} \\bigtriangledown C \\\\= & \\sum_{r=1}^{N^c_r} \\sum_{c=1}^{N^c_c} \\frac {\\partial C[r][c]} {\\partial w^x} \\bigtriangledown C \\\\ =& \\sum_{r=1}^{N^c_r} \\sum_{c=1}^{N^c_c}  \\left[ \\begin{matrix} \\frac {\\partial C[r][c]} {\\partial W^x[1][1]} & \\frac {\\partial C[r][c]} {\\partial W^x[1][2]} & ... & \\frac {\\partial C[r][c]} {\\partial W^x[1][K]}\\\\ ... \\\\  \\frac {\\partial C[r][c]} {\\partial W^x[K][1]} & \\frac {\\partial C[r][c]} {\\partial W^x[K][2]} & ... & \\frac {\\partial C[r][c]} {\\partial W^x[1][K]}  \\end{matrix} \\right] \\bigtriangledown C[r][c] \\\\=& \\sum_{r=1}^{N^c_r} \\sum_{c=1}^{N^c_c}  \\left[ \\begin{matrix} X[r][c] & X[r][c+1] & ... &X[r][c+K] \\\\ ... \\\\  X[r+K][c] & X[r+K][c+1] & ... &X[r+K][c+K]  \\end{matrix} \\right] \\bigtriangledown C[r][c] \\\\= & \\left[ \\begin{matrix} X[1][2] & X[1][3] & X[1][4] \\\\X[2][2] & X[2][3] & X[2][4] \\\\ X[3][2] & X[3][3] & X[3][4] \\end{matrix} \\right] * -.293 + \\left[ \\begin{matrix} X[2][3] & X[2][4] & X[2][5] \\\\X[3][3] & X[3][4] & X[3][5] \\\\ X[4][3] & X[4][4] & X[4][5] \\end{matrix} \\right] * -.257 \\left[ \\begin{matrix} X[3][2] & X[3][3] & X[3][4] \\\\X[4][2] & X[4][3] & X[4][4] \\\\ X[5][2] & X[5][3] & X[5][4] \\end{matrix} \\right] * -.293 + \\left[ \\begin{matrix} X[3][3] & X[3][4] & X[3][5] \\\\X[4][3] & X[4][4] & X[4][5] \\\\ X[5][3] & X[5][4] & X[5][5] \\end{matrix} \\right] * -.257 = \\left[ \\begin{matrix} 0 & -.514 & -.586 \\\\0 & -.514 & -.586\\\\ 0 & -.514 & -.586 \\end{matrix} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\bigtriangledown W^x =& \\frac {\\partial C} {\\partial W^x_0} \\bigtriangledown C = \\sum_{r=1}^{N^c_r}\\sum_{c=1}^{N^c_c} \\bigtriangledown[r][c] = - 1.613\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Update the weights for the convolution and the output layers with their estimted gradient, using a learning rate of .5, and then compute the new output value $\\hat{y}$ using the updated weight matrices. Verify that the new $\\hat{y}$ reduced the output loss function, as compared to the previous $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "W^x = W^x - .5 * \\bigtriangledown{W^x} = \\left[ \\begin{matrix} .2 & .356 & .007 \\\\ .1 & .456 & .107 \\\\ .3 & .657 & -.193 \\end{matrix} \\right]\n",
    "\\end{align} \n",
    "\n",
    "\\begin{align}\n",
    "W^x_0 = W^x_0 - .5 * \\bigtriangledown{W^x_0} = .806\n",
    "\\end{align} \n",
    "\n",
    "\\begin{align}\n",
    "W^o = W^o - .5 * \\bigtriangledown{ W^o} = [0.3465, 0.2285, 0.4465, 0.5285]^{\\top}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "W^o_0 = W^o_0 - .5 * \\bigtriangledown{W^o_0} = -0.017\n",
    "\\end{align} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the convolution layer:\n",
    "\n",
    "\\begin{align}\n",
    "C = \\left[ \\begin{matrix} 0.806 & 0.727 & 2.277 & 1.406 & 0.806  \\\\0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\end{matrix} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the activation layer:\n",
    "\\begin{align}\n",
    "C = \\left[ \\begin{matrix} 0.806 & 0.727 & 2.277 & 1.406 & 0.806  \\\\0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\\\ 0.806 & 0.727 & 2.277 & 1.406 & 0.806 \\end{matrix} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the max pooling:\n",
    "\n",
    "\\begin{align}\n",
    "P = \\left[ \\begin{matrix} 2.277 & 2.277  \\\\ 2.277 & 2.277  \\end{matrix} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After flatten:\n",
    "    \n",
    "\\begin{align}\n",
    "\\vec{P} = \\left[ \\begin{matrix} 2.277 & 2.277  & 2.277 & 2.277  \\end{matrix} \\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fully connected layer:\n",
    "\n",
    "$$z = 3.51285$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sigmoid function:\n",
    "    \n",
    "$$\\hat{y} = \\frac {\\exp( z )} { 1+\\exp(z)} = 0.97105119$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new loss:\n",
    "$$\\mathcal{L} = - \\log( \\hat{y} ) = 0.0293761$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
