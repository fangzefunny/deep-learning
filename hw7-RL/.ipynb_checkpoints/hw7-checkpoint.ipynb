{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1\n",
    "\n",
    "Follow the room example, perform Q learning using the deterministic model-free equation to update the Q function for the room configuration below. Use the same reward and Q function for the room configuration below. Use the same reward and Q function initialization, Note the goal is to find the optimal policy that produces the shorest path to outside from each room.\n",
    "\n",
    "* Identify the states, provide a generic reward function, and initial Q function \n",
    "* Follow the value-iteration pseudo code in the lectures note to enumerate each state and action, show the Q function after each iteration, and produce the final learnt policy $pi$ with respect to each state, ie $a = \\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the state:\n",
    "\n",
    "* For the state: $s0 = 0$ room, $s1 = 1$ room .....\n",
    "* For the action: $a0 =$ going to 0 room, $a1 =$ goting to the 1 room ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward function: \n",
    "\n",
    "$$R = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 100 & -1 & -1 & -1 \\\\ s1 & 100 & -1 & 0 & 0 \\\\ s2 & -1 & 0 & -1 & 0 \\\\ s3 & -1 & 0 & 0 & -1 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Q function:\n",
    "    \n",
    "$$Q^0 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 0& 0 & 0 & 0 \\\\ s1 & 0 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration:\n",
    "\n",
    "Policy: $\\pi(s) =  \\max_{a} Q(s,a)$\n",
    "\n",
    "GAME 1, start at $s_1$ \n",
    "\n",
    "* Iteration 1: $a0 = \\pi(s_1)$, $a0 = \\pi(s_0)$ (sample from uniform distribution, due to all value are the same) \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_1, \\pi(s_1) =& R(s_1, \\pi(s_1) ) + \\gamma *  Q( s_0, \\pi(s_0) ) \\\\ Q(s_1, a_0) =& 100 + .8 * 0 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^1 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 0& 0 & 0 & 0 \\\\ s1 & 100 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$\n",
    "\n",
    "\n",
    "* Iteration 2: $a0 = \\pi(s_1)$, $a0 = \\pi(s_0)$ (sample from uniform distribution, due to all value are the same) \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_0, \\pi(s_0) =& R(s_0, \\pi(s_0) ) + \\gamma *  Q( s_0, \\pi(s_0) ) \\\\ Q(s_1, a_0) =& 100 + .8 * 100\n",
    "\\end{align}\n",
    "\n",
    "$$Q^2 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 180& 0 & 0 & 0 \\\\ s1 & 100 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAME 2, start at $s_2$\n",
    "\n",
    "* Iteration 3: $a3 = \\pi(s_2)$ (sample from uniform distribution, due to all value are the same) \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_2, \\pi(s_2) =& R(s_2, \\pi(s_2) ) + \\gamma *  Q( s_3, \\pi(s_3) ) \\\\ Q(s_2, a_3) =& 0 + .8 * 0 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^3 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 180& 0 & 0 & 0 \\\\ s1 & 100 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 4: $a1 = \\pi(s_3)$ (sample from uniform distribution, due to all value are the same) \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_3, \\pi(s_3) =& R(s_3, \\pi(s_3) ) + \\gamma *  Q( s_1, \\pi(s_1) ) \\\\ Q(s_3, a_1) =& 0 + .8 * Q(s_1,a_0) \\\\ =& 0  +.8 * 100 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^4 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 180& 0 & 0 & 0 \\\\ s1 & 100 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 80 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 5:\n",
    "\n",
    "\\begin{align}\n",
    "Q(s_1, \\pi(s_1) =& R(s_1, \\pi(s_1) ) + \\gamma *  Q( s_0, \\pi(s_1) ) \\\\ Q(s_1, a_0) =& 100 + .8 * 180 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^5 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 180& 0 & 0 & 0 \\\\ s1 & 244 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 80 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 6:\n",
    "\n",
    "\\begin{align}\n",
    "Q(s_0, \\pi(s_0) =& R(s_0, \\pi(s_0) ) + \\gamma *  Q( s_0, \\pi(s_0) ) \\\\ Q(s_1, a_0) =& 100 + .8 * 180 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^6 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 244& 0 & 0 & 0 \\\\ s1 & 244 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 80 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAME 3, start at $s_2$\n",
    "\n",
    "* Iteration 7: $a3 = \\pi(s_2)$ (sample from uniform distribution, due to all value are the same) \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_2, \\pi(s_2) =& R(s_2, \\pi(s_2) ) + \\gamma *  Q( s_3, \\pi(s_3) ) \\\\ Q(s_2, a_3) =& 0 + .8 * 80 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^7 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 244& 0 & 0 & 0 \\\\ s1 & 244 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 64 \\\\ s3 & 0 & 80 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 8: \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_3, \\pi(s_3) =& R(s_3, \\pi(s_3) ) + \\gamma *  Q( s_1, \\pi(s_1) ) \\\\ Q(s_3, a_1) =& 0 + .8 * Q(s_1,a_0) \\\\ =& 0  +.8 * 244 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^8 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 244& 0 & 0 & 0 \\\\ s1 & 244 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 64 \\\\ s3 & 0 & 195 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 9: \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_1, \\pi(s_1) =& R(s_1, \\pi(s_1) ) + \\gamma *  Q( s_0, \\pi(s_0) ) \\\\ Q(s_3, a_1) =& 100 + .8 * Q(s_0,a_0) \\\\ =& 100  +.8 * 244 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^9 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 244& 0 & 0 & 0 \\\\ s1 & 295 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 64 \\\\ s3 & 0 & 195 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iteration 10: \n",
    "\n",
    "\\begin{align}\n",
    "Q(s_0, \\pi(s_0) =& R(s_0, \\pi(s_0) ) + \\gamma *  Q( s_0, \\pi(s_0) ) \\\\ Q(s_0, a_0) =& 100 + .8 * Q(s_0,a_0) \\\\ =& 100  +.8 * 244 \n",
    "\\end{align}\n",
    "\n",
    "$$Q^{10}= \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 295& 0 & 0 & 0 \\\\ s1 & 295& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 64 \\\\ s3 & 0 & 195 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to stop here. Basically the policy has been converged, but the value function is not, the optimal value function would be:\n",
    "\n",
    "$$Q^{*}= \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 500& 0 & 0 & 0 \\\\ s1 & 500& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 320 \\\\ s3 & 0 & 400 & 0 & 0 \\end{matrix} \\right]$$\n",
    "\n",
    "I get this equation because $Q(s_0, \\pi(s_0)) = 100 + .8 * Q(s_0, \\pi(s_0))$. From this example, we can see the value of the discount factor $\\gamma = .8$. Because without shi discount factor, the $Q(s_0, \\pi(s_0)$ will keep increasing and never converge. \n",
    "\n",
    "Now we can talk about the policy: \n",
    "\n",
    "Unfortunately, we may find that this $Q^10$ value function is already converge with $a_0 = \\pi(s_1)$, $a_1 = \\pi(s_3)$, $a_3 = \\pi(s_2)$.\n",
    "The problem is, as we know, the optimal convergence should be $a_0 = \\pi(s_1)$, $a_1 = \\pi(s_3)$, $a_1 = \\pi(s_2)$. If iteration 7 sample $a_1$\n",
    "\n",
    "\\begin{align}\n",
    "Q(s_2, \\pi(s_2) =& R(s_2, \\pi(s_2) ) + \\gamma *  Q( s_1, \\pi(s_1) ) \\\\ Q(s_2, a_3) =& 0 + .8 * 244\n",
    "\\end{align}\n",
    "\n",
    "$$Q^{7'} = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 244& 0 & 0 & 0 \\\\ s1 & 244 & 0 & 0 & 0 \\\\ s2 & 0 & 195 & 0 & 0 \\\\ s3 & 0 & 80 & 0 & 0 \\end{matrix} \\right]$$\n",
    "\n",
    "The world will be totally different, if we can do this sample. This is the problem of using the deterministic policy all the time, it may lose the chance to get to the optimal policy. And this should be solved by implementing a slightly stochastic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans to Q1 \n",
    "\n",
    "Learnt policy (not optimal due to sampling):\n",
    "\n",
    "* At room 0: go to the room 0 \n",
    "* At room 1: go to the room 0\n",
    "* At room 2: go to the room 3\n",
    "* At room 3: go to the room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial policy weight\n",
    "\n",
    "$p(a|s,w) = \\text{softmax}( W[s] )$$\n",
    "\n",
    "$$W^0 = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 0& 0 & 0 & 0 \\\\ s1 & 0 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAME 1, start at $s_1 \\rightarrow s_0$\n",
    "\n",
    "Notation:\n",
    "$p(\\tau_{i,t}|w)$, the first subscript is the game_i, the second is the timestep\n",
    "\n",
    "1. $s_1 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{1,1}|w) = \\log p( a_0 | s_1, w ) + \\log p( a_0 | s_0, w )$\n",
    "* $R(\\tau_{1}) = r_{1,1} = 100$\n",
    "\n",
    "2. $s_0 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{1,2}|w) = \\log p( a_0 | s_1, w ) + \\log p( a_0 | s_0, w )$\n",
    "* $R(\\tau_{1}) = r_{1,2} = 200$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAME 2, start at $s_2 \\rightarrow s_3 \\rightarrow s_1 \\rightarrow s_0$\n",
    "\n",
    "1. $s_2 \\rightarrow s_3$\n",
    "\n",
    "* $\\log p(\\tau_{2,1}|w) = p( a_3 | s_2, w )$\n",
    "* $R(\\tau_{2,1}) = r_{2,1}= 0$\n",
    "\n",
    "2. $s_3 \\rightarrow s_1$\n",
    "\n",
    "* $\\log p(\\tau_{2,2}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w)$\n",
    "* $R(\\tau_2) = \\sum_{i=1}^2 r_{2,i}= 0$\n",
    "\n",
    "3. $s_1 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{2,3}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w) + p( a_0 | s_1, w)$\n",
    "* $R(\\tau_2) = \\sum_{i=1}^3 r_{2,i}= 0+0+100 = 100$\n",
    "\n",
    "4. $s_0 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{2,3}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w) + p( a_0 | s_1, w) + \\log p( a_0 | s_0, w )$\n",
    "* $R(\\tau_2) = \\sum_{i=1}^4 r_{2,i}= 0+0+100 +100 = 200$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAME 3, start at $s_2 \\rightarrow s_3 \\rightarrow s_1 \\rightarrow s_0$\n",
    "\n",
    "1. $s_2 \\rightarrow s_3$\n",
    "\n",
    "* $\\log p(\\tau_{3,1}|w) = p( a_3 | s_2, w )$\n",
    "* $R(\\tau_{3,1}) = r_{3,1}= 0$\n",
    "\n",
    "2. $s_3 \\rightarrow s_1$\n",
    "\n",
    "* $\\log p(\\tau_{3,2}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w)$\n",
    "* $R(\\tau_3) = \\sum_{i=1}^2 r_{3,i}= 0$\n",
    "\n",
    "3. $s_1 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{3,3}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w) + p( a_0 | s_1, w) $\n",
    "* $R(\\tau_3) = \\sum_{i=1}^3 r_{3,i}= 0+0+100 = 100$\n",
    "\n",
    "4. $s_0 \\rightarrow s_0$\n",
    "\n",
    "* $\\log p(\\tau_{3,3}|w) = p( a_3 | s_2, w ) + p(a_1|s_3,w) + p( a_0 | s_1, w) + \\log p( a_0 | s_0, w )$\n",
    "* $R(\\tau_3) = \\sum_{i=1}^4 r_{3,i}= 0+0+100 +100 = 200$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these 3 trajectories:\n",
    "\n",
    "$$R^0(w) = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 0& 0 & 0 & 0 \\\\ s1 & 0 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]$$\n",
    "\n",
    "\\begin{align}\n",
    "R(w) =& \\sum_i^3 \\log p(\\tau_i|w)*R(\\tau_i) \\\\ =&  \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & \\log(a_0|s_0,w)*(200)& 0 & 0 & 0 \\\\ s1 & \\log(a_0|s_1,w)*(100+200)& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 0 \\\\ s3 & 0 & 0 & 0 & 0 \\end{matrix} \\right]  \\\\+& \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & \\log(a_0|s_0,w)*(200)& 0 & 0 & 0 \\\\ s1 & \\log(a_0|s_1,w)*(100+200)& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & \\log(a_3|s_2,w)*( 0  +0 + 100 +200) \\\\ s3 & 0 & \\log(a_1|s_3,w)*(0 +100 +200) & 0 & 0 \\end{matrix} \\right] \\\\+& \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & \\log(a_0|s_0,w)*(200)& 0 & 0 & 0 \\\\ s1 & \\log(a_0|s_1,w)*(100+200)& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & \\log(a_3|s_2,w)*( 0  +0 + 100 +200) \\\\ s3 & 0 & \\log(a_1|s_3,w)*(0 +100 +200) & 0 & 0 \\end{matrix} \\right] \\\\=& \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & 3*\\log(a_0|s_0,w)*200& 0 & 0 & 0 \\\\ s1 & 3* \\log(a_0|s_1,w)*300 & 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & 2*\\log(a_3|s_2,w)*300 \\\\ s3 & 0 & 2*\\log(a_1|s_3,w)*300 & 0 & 0 \\end{matrix} \\right] \n",
    "\\end{align}\n",
    "\n",
    "**Determinic policy**\n",
    "\n",
    "To maximize this: $w^* = \\arg\\max_w R(w) = \\left[ \\begin{matrix} S & a0 & a1 & a2 & a3  \\\\ s0 & \\infty& 0 & 0 & 0 \\\\ s1 & \\infty& 0 & 0 & 0 \\\\ s2 & 0 & 0 & 0 & \\infty \\\\ s3 & 0 & \\infty & 0 & 0 \\end{matrix} \\right]$\n",
    "\n",
    "So the policy is:\n",
    "\n",
    "* At room 0: go to the room 0 \n",
    "* At room 1: go to the room 0\n",
    "* At room 2: go to the room 3\n",
    "* At room 3: go to the room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can get the same policy with Q1. But note that, there I control the sampling, ensuring the trajectories are  exactly the same with Q1. If we make another sampling, where at state 2, the agent choose to go to room1, the policy we got from Q2 might be different from Q1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
