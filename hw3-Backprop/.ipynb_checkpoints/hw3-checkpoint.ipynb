{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ANN contains three layers: input layer ($\\text{X}^{N\\times1}$), a hidden layer ($\\text{H}^{N_1 \\times 1}$), and an output layer ($\\hat{ \\text{Y} }^{K \\times 1}$). Let $\\text{W}^1$ and $\\text{W}^1_0$ respectiely represent the weight matrix and weight bias vector for the hidden layer, and $\\text{W}^2$ and $\\text{W}^2_o$ be the weight matrix and weight bias vector for the output layer respectively. Assuming the ReLU activation function for the hidden layer and softmax function for the ouput layer, derive the expression for $\\frac {\\partial \\hat{\\text{Y}}} {\\partial \\text{W}^1}$ and $\\frac {\\partial \\hat{\\text{Y}}} {\\partial \\text{W}^1_{i,j}}$, where $\\text{W}^1_{i,j}$ represents ith row and jth column element of $\\text{W}^1$. Show the derivation process and intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. For $\\frac {\\partial \\hat{\\text{Y}}} {\\partial \\text{W}^1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\text{Z}^i\\in \\mathbb{R}^{N_i\\times1}$ the output of $(\\text{W}^i)^{\\top}\\text{sth}$. $\\text{sth}$ can be either $X$ or $H$.      \n",
    "\n",
    "**The forward model:**\n",
    "\n",
    "1. $Z^1 = (\\text{W}^1)^{\\top}X + W^1_0$, where $Z^1 \\in \\mathbb{R}^{N_1 \\times 1}, \\text{W}^1 \\in \\mathbb{R}^{N \\times N_1}, \\text{W}_0^1 \\in \\mathbb{R}^{N_1 \\times1}$\n",
    "2. $H = \\max( 0, Z^1 )$, where $H \\in \\mathbb{R}^{N_1 \\times 1}$\n",
    "3. $Z^2 = (\\text{W}^2)^{\\top}H + W^2_0$, where $Z^2 \\in \\mathbb{R}^{K \\times 1},  \\text{W}^2 \\in \\mathbb{R}^{N_1 \\times K}, \\text{W}_0^2 \\in \\mathbb{R}^{K \\times1}$\n",
    "4. $\\hat{\\text{Y}} = \\sigma_M(Z^2) = \\left[ \\begin{matrix} \\exp(Z^2[1]) / \\sum^K_{k'=1} \\exp(Z^2[k'])   \\\\ ...\\\\ \\exp(Z^2[K]) / \\sum^K_{k'=1} \\exp(Z^2[k'])  \\end{matrix} \\right]$, where $\\hat{\\text{Y}} \\in \\mathbb{R}^{K \\times 1}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The backward process:**\n",
    "\n",
    "$\\frac {\\partial{\\hat{\\text{Y}}}} {\\partial \\text{W}^1} = \\frac {\\partial Z^1} {\\partial W^1} \\frac {\\partial H} {\\partial Z^1} \\frac {\\partial Z^2} {\\partial H} \\frac {\\partial \\hat{Y}} {\\partial Z^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\frac {\\partial \\hat{Y}} {\\partial Z^2}$ \n",
    "\n",
    "    This is a $K \\times K$ matrix: $\\left[ \\begin{matrix} \\frac {\\partial\\hat{Y}[1]} {\\partial Z^2} & ... & \\frac {\\partial\\hat{Y}[K]} {\\partial Z^2}  \\end{matrix} \\right]^{1 \\times K}$\n",
    "    \n",
    "    For $\\frac {\\partial\\hat{Y}[i]} {\\partial Z^2} = \\left[ \\begin{matrix} \\frac {\\partial \\hat{Y}[i]} {\\partial Z^2[1]} \\\\ ...\\\\ \\frac {\\partial\\hat{Y}[i]} {\\partial Z^2 [K]} \\end{matrix} \\right]^{K \\times 1}$\n",
    "    \n",
    "    For $\\frac {\\partial\\hat{Y}[i]} {\\partial Z^2[j]}$, if $i=j: \\frac {\\partial\\hat{Y}[i]} {\\partial Z^2[j]} = \\sigma_M(z[i])( 1 - \\sigma_M(z[i]))$, else: $ - \\sigma_M(z[i])\\sigma_M(z[j])$\n",
    "    \n",
    "    So the $K \\times K$ derivative matrix isï¼š\n",
    "    \n",
    "    $$\\left[ \\begin{matrix} \\sigma_M(Z^2[1])( 1 - \\sigma_M(Z^2[1])) & -\\sigma_M(Z^2[1])\\sigma_M(Z^2[2])) &...& -\\sigma_M(Z^2[K])\\sigma_M(Z^2[K])) \\\\ -\\sigma_M(Z^2[2])\\sigma_M(Z^2[1])) & \\sigma_M(Z^2[2])( 1 - \\sigma_M(Z^2[2])) & ... & -\\sigma_M(Z^2[K])\\sigma_M(Z^2[K])) \\\\ ... \\\\ -\\sigma_M(Z^2[K])\\sigma_M(Z^2[1])) & -\\sigma_M(Z^2[K])\\sigma_M(Z^2[2])) & ... & \\sigma_M(Z^2[K])( 1 - \\sigma_M(Z^2[K])) \\end{matrix} \\right]^{K \\times K}$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\frac {\\partial Z^2} {\\partial H}$ \n",
    "\n",
    "    This is a $N_1 \\times K$ matrix: $W^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\frac {\\partial H} {\\partial Z^1}$ \n",
    "\n",
    "    This is a $N_1 \\times 1$ vector: $\\left[ \\begin{matrix} \\frac {\\partial H[1]} {\\partial Z^1[1]} \\\\ ... \\\\ \\frac {\\partial H[N_1]} {\\partial Z^1[N_1]}  \\end{matrix} \\right]^{N_1 \\times 1}$\n",
    "    \n",
    "    For each $\\frac {\\partial H[i]} {\\partial Z^1[i]}$, if $Z^{i} > 0, \\frac {\\partial H[i]} {\\partial Z^1[i]} =1$, else $\\frac {\\partial H[i]} {\\partial Z^1[i]}= 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $\\frac {\\partial Z^1} {\\partial W^1}$\n",
    "\n",
    "    This is a $N \\times N_1 \\times N_1 $ tensor: $\\left[ \\begin{matrix} \\frac {\\partial Z^1[1]} {\\partial W^1} & ... & \\frac {\\partial Z^1[N_1]} {\\partial W^1}  \\end{matrix} \\right]^{1 \\times N_1}$\n",
    "    \n",
    "    At $\\frac {\\partial Z^1[i]} {\\partial W^1} = \\left[ \\begin{matrix} \\frac {\\partial Z^1[i]} {\\partial W^1[1]} & ... & \\frac {\\partial Z^1[i]} {\\partial W^1[N_1]}  \\end{matrix} \\right]^{1 \\times N_1}$\n",
    "    \n",
    "    For $\\frac {\\partial Z^1[i]} {\\partial W^1[j]}$ if i = j, $\\frac {\\partial Z^1[i]} {\\partial W^1[j]} = X^{N \\times 1}$, else $\\frac {\\partial Z^1[i]} {\\partial W^1[j]} = 0^{N\\times1}$\n",
    "    \n",
    "    So the $N \\times N_1 \\times N_1 $ tensor is: \n",
    "    \n",
    "    $$\\left[ \\left[ \\begin{matrix} X^{N\\times1} & 0^{N\\times1} & ... & 0^{N\\times1}\\end{matrix} \\right]^{N \\times N_1}, \\left[ \\begin{matrix} 0^{N\\times1} & X^{N\\times1} &... & 0^{N\\times1}\\end{matrix} \\right]^{N \\times N_1}, \\left[ \\begin{matrix} 0^{N\\times1} & 0^{N\\times1} & ... & X^{N\\times1}\\end{matrix} \\right]^{N \\times N_1} \\right]^{N \\times N_1 \\times N_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To multiply them:\n",
    "\n",
    "$$\\frac {\\partial \\hat{Y}} {\\partial W^1}  = \\frac {\\partial Z^1} {\\partial W^1} \\frac {\\partial H} {\\partial Z^1} W^2 \\frac {\\partial \\hat{Y}} {\\partial Z^2}$$\n",
    "\n",
    "The dim is:\n",
    "\n",
    "\\begin{align}\n",
    "&(N \\times N_1 \\times N_1) \\times (N_1 \\times 1) ( N_1 \\times K ) ( K \\times K ) \\\\ =& ( N \\times N_1 ) (N_1 \\times K ) ( K \\times K ) \\\\= & N \\times K \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. For $\\frac {\\partial \\hat{\\text{Y}}} {\\partial \\text{W}^1_{i,j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward model**\n",
    "\n",
    "1. $z^1[j] = w^1[i][j]x[i] + w^1_{0}[j]$, where $z^1[j] \\in \\mathbb{R}$\n",
    "2. $h^1[j] = \\max( 0, z^1[j])$, where $h^1[j] \\in \\mathbb{R}$\n",
    "3. $Z^2 = w^2[j][]h^1[j] + w^2_{0}[]$, where $Z_k^2 \\in \\mathbb{R}^{K \\times 1}$ \n",
    "4. $\\hat{Y}[k]= \\sigma_M( Z^2 )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The backward process:**\n",
    "\n",
    "because this $W^1_{ij}$ is a scalar, as we mentioned in the class, we should change the sequence of chain rule\n",
    "\n",
    "$\\frac {\\partial{\\hat{\\text{Y}}}} {\\partial \\text{W}^1_{i,j}} =   \\frac {\\partial \\hat{Y}} {\\partial Z^2}\\frac {\\partial Z^2} {\\partial H_j} \\frac {\\partial H_j} {\\partial Z_j^1}\\frac {\\partial Z_j^1} {\\partial W_{i,j}^1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\frac {\\partial \\hat{Y}} {\\partial Z^2}$ the same with what we mentioned, $K \\times K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\frac {\\partial Z^2} {\\partial H_j}$ is a $K \\times 1$ vector $(W^2_j)^{\\top}$ which is the transpose of row of matrix $W^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\frac {\\partial H_j} {\\partial Z_j^1}$, if $Z_j > 0, \\frac {\\partial H_j} {\\partial Z_j^1} =1$, else $\\frac {\\partial H_j} {\\partial Z_j^1}=0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $\\frac {\\partial Z_j^1} {\\partial W_{i,j}^1} = x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To multiply them:\n",
    "\n",
    "$\\frac {\\partial{\\hat{\\text{Y}}}} {\\partial \\text{W}^1_{i,j}} =   \\frac {\\partial \\hat{Y}} {\\partial Z^2} (W^2_j)^{\\top} \\frac {\\partial H_j} {\\partial Z_j^1}x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dim: $(K \\times K) \\times (K \\times 1) \\times 1 \\times 1 = (K \\times 1)$\n",
    "\n",
    "This is consistent with our vector by scalar derivative convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "The structure of a Neural Network is given below\n",
    "The input value $X = \\left[ \\begin{matrix} X_1 \\\\X_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 1 \\\\ 0 \\end{matrix} \\right]$ and the desired output value is $Y = \\left[ \\begin{matrix} Y_1  \\\\ Y_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 0 \\\\ 1 \\end{matrix} \\right]$. The initial weight matrix for the first layer $W^1$:\n",
    "\n",
    "$$W^1 = \\left[ \\begin{matrix} W^1_1  & W^1_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 3 & 6  \\\\ 4 & 5 \\end{matrix} \\right]$$\n",
    "\n",
    "The bias weight matrix $W^1_0 = \\left[ \\begin{matrix} 1  \\\\ -6 \\end{matrix} \\right]$. The initial weight matrix for the second layer $W^2$:\n",
    "\n",
    "$$W^2 = \\left[ \\begin{matrix} W^2_1  & W^2_2 \\end{matrix} \\right] = \\left[ \\begin{matrix} 2 & 3  \\\\ 4 & 3 \\end{matrix} \\right]$$\n",
    "\n",
    "The bias weight matrix $W^2_0  = \\left[ \\begin{matrix} -1 \\\\ -2 \\end{matrix} \\right] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following tasks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation: calculate the value of the hidden nodes using sigmoid function as the activation function and obtain the predicted value for output nodes $\\hat{Y}$ using softmax as the ouput function. Compute the gradient of the output $\\bigtriangledown \\hat{Y}$ using cross-entropy loss function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation:**\n",
    "1. $Z^1 = (\\text{W}^1)^{\\top}X + W^1_0$, where $Z^1 \\in \\mathbb{R}^{N_1 \\times 1}, \\text{W}^1 \\in \\mathbb{R}^{N \\times N_1}, \\text{W}_0^1 \\in \\mathbb{R}^{N_1 \\times1}$\n",
    "\n",
    "    $Z^1 = \\left[ \\begin{matrix} 3 \\\\ 6 \\end{matrix} \\right] +  \\left[ \\begin{matrix} 1 \\\\ -6 \\end{matrix} \\right] =  \\left[ \\begin{matrix} 4 \\\\ 0 \\end{matrix} \\right]$\n",
    "\n",
    "2. $H = \\sigma(Z^1) =  {\\exp(Z^1)} /{(1 + \\exp( Z^1))}  $, where $H \\in \\mathbb{R}^{N_1 \\times 1}$\n",
    "\n",
    "    $H = \\left[ \\begin{matrix} \\exp(4) / ( 1+ \\exp(4)) \\\\ \\exp(0) /(1+\\exp(0) \\end{matrix} \\right] =  \\left[ \\begin{matrix} .982 \\\\ .500 \\end{matrix} \\right]$\n",
    "\n",
    "3. $Z^2 = (\\text{W}^2)^{\\top}H + W^2_0$, where $Z^2 \\in \\mathbb{R}^{K \\times 1},  \\text{W}^2 \\in \\mathbb{R}^{N_1 \\times K}, \\text{W}_0^2 \\in \\mathbb{R}^{K \\times1}$\n",
    "\n",
    "    $Z^2 = \\left[ \\begin{matrix} 3.964 \\\\ 4.446 \\end{matrix} \\right] +  \\left[ \\begin{matrix} -1 \\\\ -2 \\end{matrix} \\right] =  \\left[ \\begin{matrix} 2.964 \\\\ 2.446 \\end{matrix} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $\\hat{\\text{Y}} = \\sigma_M(Z^2) = \\left[ \\begin{matrix} \\exp(Z^2[1]) / \\sum^K_{k'=1} \\exp(Z^2[k'])   \\\\ ...\\\\ \\exp(Z^2[K]) / \\sum^K_{k'=1} \\exp(Z^2[k'])  \\end{matrix} \\right]$, where $\\hat{\\text{Y}} \\in \\mathbb{R}^{K \\times 1}$ \n",
    "\n",
    "    $\\hat{\\text{Y}} = \\left[ \\begin{matrix} .627 \\\\ .373 \\end{matrix} \\right]$\n",
    "\n",
    "5. $\\mathcal{L}( Y, \\hat{Y}) = -Y^{\\top} \\log \\hat{Y}$\n",
    "\n",
    "    $\\mathcal{L}( Y, \\hat{Y})  = .985$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $\\hat{Y}$**: $\\bigtriangledown {\\hat{Y}}$\n",
    "\n",
    "$\\bigtriangledown {\\hat{Y}} = - \\frac {\\partial \\mathcal{L}( Y, \\hat{Y})} {\\partial \\hat{Y}} = -\\frac {Y} {\\hat{Y}}  = \\left[ \\begin{matrix} 0 \\\\ -2.679 \\end{matrix} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $H$: $\\bigtriangledown H$**\n",
    "\n",
    "$\\bigtriangledown H = \\frac {\\partial Z^2} {\\partial H} \\frac {\\partial \\sigma(Z^2)} {\\partial Z^2}\\bigtriangledown \\hat{Y} = W^2 \\left[ \\begin{matrix} \\hat{Y}[1]( 1 - \\hat{Y}[1]) &  -\\hat{Y}[1] \\hat{Y}[2] \\\\ -\\hat{Y}[2] \\hat{Y}[1] & \\hat{Y}[2]( 1 - \\hat{Y}[2]) \\end{matrix} \\right] \\left[ \\begin{matrix}- Y[1]/\\hat{Y}[1] \\\\ -Y[2]/\\hat{Y}[2] \\end{matrix} \\right] =  \\left[ \\begin{matrix} 2 & 3 \\\\ 4 & 3 \\end{matrix} \\right]  \\left[ \\begin{matrix} .627 \\\\ -.627 \\end{matrix} \\right]  = \\left[ \\begin{matrix} -.627 \\\\ .627 \\end{matrix} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $\\bigtriangledown W^2_0$**\n",
    "\n",
    "$\\bigtriangledown W^2_0 = \\frac {\\partial Z^2} {\\partial W^2_0} \\frac {\\partial \\sigma(Z^2)} {\\partial Z^2}\\bigtriangledown \\hat{Y} = 1* \\left[ \\begin{matrix} \\hat{Y}[1]( 1 - \\hat{Y}[1]) &  -\\hat{Y}[1] \\hat{Y}[2] \\\\ -\\hat{Y}[2] \\hat{Y}[1] & \\hat{Y}[2]( 1 - \\hat{Y}[2]) \\end{matrix} \\right] \\left[ \\begin{matrix}- Y[1]/\\hat{Y}[1] \\\\ -Y[2]/\\hat{Y}[2] \\end{matrix} \\right] = \\left[ \\begin{matrix} .627 \\\\ -.627 \\end{matrix} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $\\bigtriangledown W^2$**\n",
    "\n",
    "$\\bigtriangledown W^2 = \\frac {\\partial Z^2} {\\partial W^2} \\frac {\\partial \\sigma(Z^2)} {\\partial Z^2}\\bigtriangledown \\hat{Y} =H (\\left[ \\begin{matrix} \\hat{Y}[1]( 1 - \\hat{Y}[1]) &  -\\hat{Y}[1] \\hat{Y}[2] \\\\ -\\hat{Y}[2] \\hat{Y}[1] & \\hat{Y}[2]( 1 - \\hat{Y}[2]) \\end{matrix} \\right] \\left[ \\begin{matrix}- Y[1]/\\hat{Y}[1] \\\\ -Y[2]/\\hat{Y}[2] \\end{matrix} \\right])^{\\top}  = \\left[ \\begin{matrix} .982 \\\\ .5 \\end{matrix} \\right]\\left[ \\begin{matrix} .627 & -.627 \\end{matrix} \\right] = \\left[ \\begin{matrix} .616 & -.616 \\\\ .314 & -.314\\end{matrix} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $\\bigtriangledown W^1_0$**\n",
    "\n",
    "$\\bigtriangledown W^1_0 = \\frac {\\partial Z^1} {\\partial W^1_0} \\frac {\\partial \\sigma(Z^1)} {\\partial Z^1}\\bigtriangledown H = 1 * \\left[ \\begin{matrix} H[1]( 1- H[1]) \\\\ H[2]( 1- H[2]) \\end{matrix}\\right]\\bigtriangledown H = \\left[ \\begin{matrix} -.011 \\\\ .157 \\end{matrix}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of $\\bigtriangledown W^1$**\n",
    "\n",
    "$\\bigtriangledown W^1 = \\frac {\\partial Z^1} {\\partial W^1} \\frac {\\partial \\sigma(Z^1)} {\\partial Z^1}\\bigtriangledown H = X (\\left[ \\begin{matrix} H[1]( 1- H[1]) \\\\ H[2]( 1- H[2]) \\end{matrix}\\right]\\bigtriangledown H )^{\\top}= \\left[ \\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right]\\left[ \\begin{matrix} -.627 & .627 \\end{matrix}\\right]=\\left[ \\begin{matrix} -.011 & .157\\\\ 0&0 \\end{matrix}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^1 =W^1 - .5 * \\bigtriangledown W^1 = \\left[ \\begin{matrix} 3 & 6  \\\\ 4 & 5 \\end{matrix} \\right] - .5*\\left[ \\begin{matrix} -.627 & .627\\\\ 0&0 \\end{matrix}\\right] = \\left[ \\begin{matrix} 3.006 & 5.922 \\\\ 4&5 \\end{matrix}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^1_0 =W^1_0 - .5 * \\bigtriangledown W^1_0 = \\left[ \\begin{matrix} 1  \\\\ -6 \\end{matrix} \\right] - .5*\\left[ \\begin{matrix} -.011 \\\\ .157 \\end{matrix}\\right] = \\left[ \\begin{matrix} 1.006\\\\ -6.079 \\end{matrix}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^2 =W^2 - .5 * \\bigtriangledown W^2 = \\left[ \\begin{matrix} 2 & 3  \\\\ 4 & 3 \\end{matrix} \\right] - .5*\\left[ \\begin{matrix} .616 & -.616 \\\\ .314 & -.314 \\end{matrix}\\right] = \\left[ \\begin{matrix} 1.692 & 3.308 \\\\ 3.843 & 3.157 \\end{matrix}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^2_0 =W^2_0 - .5 * \\bigtriangledown W^2_0 = \\left[ \\begin{matrix} -1  \\\\ -2 \\end{matrix} \\right] - .5*\\left[ \\begin{matrix} .627 \\\\ -.627 \\end{matrix}\\right] = \\left[ \\begin{matrix} -1.627\\\\ -1.373 \\end{matrix}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Loss**\n",
    "\n",
    "$\\mathcal{L}_{\\text{new}} = .1 < .985$\n",
    "This means the gradient work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
